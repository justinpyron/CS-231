{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## To-do\n",
    "\n",
    "- Method to create an adversarial dataset\n",
    "    - input should be the filename of where the created dataset will be stored. Save as a pickle?\n",
    "- You'll need to write somewhere a function that takes as input the saved pickle and \n",
    "    creates a Pytorch dataset/dataloader from it. \n",
    "    - This will take as input the path to the pickle\n",
    "    - Optional input arguments: the original class and the target class. You'll have to do \n",
    "        some subsetting/slicing for this. Should be very straightforward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an adversarial image dataset\n",
    "\n",
    "- Have two functions that both take in the same argument: a filename where dataset will be stored\n",
    "    - One that writes/creates dataset\n",
    "    - One that reads the saved file and returns something -- either a numpy array or a pytorch dataloader\n",
    "- For the function that reads the dataset, have an optional arugment be a (original, target) pair, and it will return only images whose true label is original and whose adversary causes the model to predict target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adversarial_utils:\n",
    "    '''\n",
    "    Class that helps to experiment with adversarial \n",
    "    image generation. Also contains functionality to \n",
    "    create an adversarial image dataset\n",
    "    '''\n",
    "    def __init__(self, dataset):\n",
    "        assert (dataset == 'cifar' or dataset == 'mnist'), \\\n",
    "            'Dataset must either be \\'cifar\\' or \\'mnist\\''\n",
    "        self.dataset = dataset\n",
    "        self.data = utils.get_data(dataset)\n",
    "        self.model = utils.trained_model(dataset)\n",
    "    \n",
    "    def fool_calibrate(self, \n",
    "                       target, \n",
    "                       epsilon=1e-2, \n",
    "                       num_iters=100, \n",
    "                       max_L2_norm=None, \n",
    "                       drop_failures=True,\n",
    "                       verbose=True):\n",
    "        '''\n",
    "        Generate a set of adversarial images from a random batch of data\n",
    "        '''\n",
    "        images, labels = iter(self.data).next()\n",
    "        images, labels = images.clone(), labels.clone()\n",
    "        \n",
    "        self.calibrate_data = self.fool(images, \n",
    "                                        labels, \n",
    "                                        target, \n",
    "                                        epsilon=epsilon, \n",
    "                                        num_iters=num_iters, \n",
    "                                        max_L2_norm=max_L2_norm,\n",
    "                                        calibrate=True,\n",
    "                                        drop_failures=drop_failures, \n",
    "                                        verbose=verbose)\n",
    "        self._visualize(target)\n",
    "\n",
    "    def _visualize(self, target):\n",
    "        '''\n",
    "        Visualize the adversarial images and perturbations generated \n",
    "        with fool_calibrate()\n",
    "        '''\n",
    "        a,b,c,d,e,f,g = self.calibrate_data\n",
    "        original_images, pert_images, adv_images = a, b, c\n",
    "        original_labels, target_labels = d, e\n",
    "        required_iters, pert_norm = f,g\n",
    "        predictions = torch.argmax(self.model(adv_images),1)\n",
    "\n",
    "        for i in range(len(adv_images)):\n",
    "            pred = predictions[i].clone()\n",
    "            label = original_labels[i].clone()\n",
    "            required_iter = required_iters[i].clone()\n",
    "            norm = pert_norm[i].clone()\n",
    "            adv_im = adv_images[i].clone()\n",
    "            pert_im = pert_images[i].clone()\n",
    "            original_im = original_images[i].clone()\n",
    "            \n",
    "            # In order to plot, normalize so pixels are in range [0,1]\n",
    "            pert_im = pert_im - pert_im.min()\n",
    "            pert_im = pert_im/pert_im.max()\n",
    "            \n",
    "            if self.dataset == 'cifar':\n",
    "                _cmap = None\n",
    "                adv_im = adv_im.squeeze(0).numpy().transpose((1,2,0))\n",
    "                pert_im = pert_im.squeeze(0).numpy().transpose((1,2,0))\n",
    "                original_im = original_im.squeeze(0).numpy().transpose((1,2,0))\n",
    "            \n",
    "            if self.dataset == 'mnist':\n",
    "                _cmap = 'binary_r'\n",
    "                adv_im = adv_im.squeeze(0).numpy()\n",
    "                pert_im = pert_im.squeeze(0).numpy()\n",
    "                original_im = original_im.squeeze(0).numpy()\n",
    "            \n",
    "            print('Original Label: {}'.format(label))\n",
    "            print('Model prediction: {}'.format(pred))\n",
    "\n",
    "            # Plot adversary\n",
    "            plt.subplot(1,3,1)\n",
    "            plt.imshow(adv_im, cmap=_cmap)\n",
    "            plt.title('Adversary')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            # Plot perturbation\n",
    "            plt.subplot(1,3,2)\n",
    "            plt.imshow(pert_im, cmap=_cmap)\n",
    "            plt.title('Iters: {:.0f}, Norm: {:.3f}'.format(required_iter, norm))\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            # Plot original\n",
    "            plt.subplot(1,3,3)\n",
    "            plt.imshow(original_im, cmap=_cmap)\n",
    "            plt.title('Original')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            plt.show()\n",
    "            print('\\n')\n",
    "\n",
    "            \n",
    "    def fool(self, \n",
    "             images, \n",
    "             labels, \n",
    "             target, \n",
    "             epsilon=5e-2, \n",
    "             num_iters=100, \n",
    "             max_L2_norm=None,\n",
    "             calibrate=False,\n",
    "             drop_failures=True,\n",
    "             verbose=False):\n",
    "        '''\n",
    "        Generated adversarial images for a batch of data contained in argument images\n",
    "        '''\n",
    "        \n",
    "        start = time.time()  # Keep track of how long this takes\n",
    "        num = len(images)\n",
    "\n",
    "        # Take a subset of batch:\n",
    "        # - only keep images that are correctly classified\n",
    "        # - only keep images not already belonging to target class\n",
    "        pred = torch.argmax(self.model(images), 1)\n",
    "        mask = (pred == labels) * (labels != target)\n",
    "        images, labels = images[mask], labels[mask]\n",
    "        original_images, original_labels = images.clone(), labels.clone()\n",
    "\n",
    "        images.requires_grad_(True)  # very important!\n",
    "        # We don't need the gradients of anything else\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        perturbation = torch.zeros_like(images)\n",
    "        required_iters = torch.ones_like(labels).type(torch.float)\n",
    "        fooled = False\n",
    "        iteration = 0\n",
    "\n",
    "        # Initialize upstream gradients\n",
    "        dout = torch.zeros_like(self.model(images), dtype=torch.float)\n",
    "        dout[:,target] = 1.  # only compute gradient w.r.t. target class\n",
    "\n",
    "        while fooled is False and iteration < num_iters:\n",
    "            output = self.model(images)\n",
    "            self.model.zero_grad() # zero out gradients so they don't accumulate\n",
    "            grad = torch.autograd.grad(outputs=output, \n",
    "                                       inputs=images, \n",
    "                                       grad_outputs=dout)[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                proposed_perturbation = epsilon * grad\n",
    "\n",
    "                # Make sure pixels in resulting image are in [0,1]\n",
    "                pert_upper_bound = torch.ones_like(images) - images\n",
    "                pert_lower_bound = - images\n",
    "                proposed_perturbation = torch.min(proposed_perturbation, pert_upper_bound)\n",
    "                proposed_perturbation = torch.max(proposed_perturbation, pert_lower_bound)\n",
    "\n",
    "                # Update images and perturbation\n",
    "                perturbation.add_(proposed_perturbation)\n",
    "                images.add_(proposed_perturbation)\n",
    "                \n",
    "                # If an example is correctly predicted, set all upward gradients\n",
    "                # flowing to that example to zero; we've successfully found an\n",
    "                # adversarial image that tricks the model and no longer need to \n",
    "                # update the original. We keep looping to try to find successful\n",
    "                # adversarial images for the other examples.\n",
    "                predictions = torch.argmax(self.model(images), 1)\n",
    "                dout[:,target] = (predictions != target)\n",
    "                required_iters.add_((predictions != target).type(torch.float))\n",
    "\n",
    "                # If every adversarial example fooled the model, we're done\n",
    "                if (predictions == target).sum() == labels.size(0):\n",
    "                    fooled = True\n",
    "                iteration += 1\n",
    "\n",
    "        num_kept = len(images)\n",
    "        num_terminated = (required_iters < num_iters).sum()\n",
    "        \n",
    "        # Only return adversarial images that meet certain criteria\n",
    "        with torch.no_grad():\n",
    "            # Only keep images if they successfully fool model\n",
    "            predictions = torch.argmax(self.model(images),1)\n",
    "            fool_success = (predictions == target)\n",
    "            num_fool_success = fool_success.sum()\n",
    "            if drop_failures:\n",
    "                mask = fool_success\n",
    "                images = images[mask]\n",
    "                labels = labels[mask]\n",
    "                perturbation = perturbation[mask]\n",
    "                original_images = original_images[mask]\n",
    "                original_labels = original_labels[mask]\n",
    "                required_iters = required_iters[mask]\n",
    "\n",
    "            # Only keep images if perturbation norms is less than max_L2_norm\n",
    "            pert_norm = torch.norm(perturbation.view(perturbation.size(0),-1), dim=1)\n",
    "            if max_L2_norm is not None:\n",
    "                mask = (pert_norm < max_L2_norm)\n",
    "                num_ok_pert = mask.sum()\n",
    "                images = images[mask]\n",
    "                labels = labels[mask]\n",
    "                perturbation = perturbation[mask]\n",
    "                original_images = original_images[mask]\n",
    "                original_labels = original_labels[mask]\n",
    "                required_iters = required_iters[mask]\n",
    "                pert_norm = pert_norm[mask]\n",
    "\n",
    "        if verbose:\n",
    "            print('Took {:.2f} seconds'.format(time.time() - start))\n",
    "            print('Number in batch that model successfully predicts: {}/{}'.format(num_kept, num))\n",
    "            print('Number that terminated before max number of iterations: {}/{}'.format(num_terminated, num))\n",
    "            if drop_failures:\n",
    "                print('Number successfully fooled: {}/{}'.format(fool_success.sum(), num))\n",
    "            if max_L2_norm is not None:\n",
    "                print('Number with small enough perturbation: {}/{}'.format(num_ok_pert, num))\n",
    "            print('\\n\\n')\n",
    "        \n",
    "        target_labels = target * torch.ones_like(original_labels)\n",
    "        if calibrate:\n",
    "            return (original_images, perturbation.detach(), images.detach(), original_labels, target_labels,\n",
    "                    required_iters, pert_norm)\n",
    "        else:\n",
    "            return (original_images, perturbation.detach(), images.detach(), original_labels, target_labels)\n",
    "        \n",
    "        \n",
    "    def make_dataset(self, \n",
    "                     file_name, \n",
    "                     num_examples,\n",
    "                     epsilon=1e-2,\n",
    "                     num_iters=100,\n",
    "                     max_L2_norm=None):\n",
    "        '''\n",
    "        Create a dataset of adversarial images\n",
    "        Arguments:\n",
    "        - file_name: name of the .npz file where dataset will be stored\n",
    "        - num_examples: number of examples to include in dataset\n",
    "        '''\n",
    "        start = time.time()\n",
    "        \n",
    "        originals = list()\n",
    "        perturbations = list()\n",
    "        adversaries = list()\n",
    "        original_labels = list()\n",
    "        target_labels = list()\n",
    "        \n",
    "        running_total = 0\n",
    "        \n",
    "        # Iterate through dataset\n",
    "        for i, (image_batch,label_batch) in enumerate(self.data):\n",
    "            print('Working on batch {}'.format(i))\n",
    "            \n",
    "            # Generate adversarial images for each class label\n",
    "            for target in range(10):\n",
    "                batch_data = self.fool(image_batch, label_batch, target, epsilon=epsilon, \n",
    "                                       num_iters=num_iters, max_L2_norm=max_L2_norm)\n",
    "                orig, pert, adv, orig_label, target_label = batch_data\n",
    "                originals.append(orig.numpy())\n",
    "                perturbations.append(pert.numpy())\n",
    "                adversaries.append(adv.numpy())\n",
    "                original_labels.append(orig_label.numpy())\n",
    "                target_labels.append(target_label.numpy())\n",
    "                \n",
    "            running_total += orig.size(0)\n",
    "            if running_total > num_examples:\n",
    "                break\n",
    "        \n",
    "        originals = np.concatenate(originals)\n",
    "        perturbations = np.concatenate(perturbations)\n",
    "        adversaries = np.concatenate(adversaries)\n",
    "        original_labels = np.concatenate(original_labels)\n",
    "        target_labels = np.concatenate(target_labels)\n",
    "        \n",
    "        arrays = {'original_images': originals,\n",
    "                  'perturbations': perturbations,\n",
    "                  'adversarial_images': adversaries,\n",
    "                  'original_labels': original_labels,\n",
    "                  'target_labels': target_labels}\n",
    "        \n",
    "        np.savez(file_name, **arrays)\n",
    "        print('Took {:.2f} minutes'.format( (time.time() - start)/60.0 ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'test'\n",
    "if '.npz' not in s:\n",
    "    s += '.npz'\n",
    "\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cifar_adv = Adversarial_utils('cifar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_adv.make_dataset('test_cifar_dataset', num_examples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adversary_Data(Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader for adversarial images\n",
    "    \"\"\"\n",
    "    def __init__(self, file_name, original_label, target_label):\n",
    "        \"\"\" Intialize the adversarial image dataset\n",
    "        Args:\n",
    "        - file_name: name of file where adversarial data is stored\n",
    "        - original_label: the original label of images\n",
    "        - target_label: the label that adversaries trick model into predicting\n",
    "        \"\"\"\n",
    "        assert (original_label != target_label), 'Target label must be different from original label!'\n",
    "        if '.npz' not in file_name:\n",
    "            file_name += '.npz'\n",
    "        data = np.load(file_name)\n",
    "        \n",
    "        original_labels = data['original_labels']\n",
    "        target_labels = data['target_labels']\n",
    "        mask = (original_labels == original_label) * (target_labels == target_label)\n",
    "\n",
    "        self.original_labels = torch.from_numpy(original_labels[mask])\n",
    "        self.target_labels = torch.from_numpy(target_labels[mask])\n",
    "        self.originals = torch.from_numpy(data['original_images'][mask])\n",
    "        self.perturbations = torch.from_numpy(data['perturbations'][mask])\n",
    "        self.adversaries = torch.from_numpy(data['adversarial_images'][mask])\n",
    "        \n",
    "        self.len = self.originals.size(0)\n",
    "        print('Lenght of dataset: {}'.format(self.len))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        original = self.originals[index]\n",
    "        pert = self.perturbations[index]\n",
    "        adv = self.adversaries[index]\n",
    "        orig_label = self.original_labels[index].item()\n",
    "        target_label = self.target_labels[index].item()\n",
    "        return original, pert, adv, orig_label, target_label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Adversary_Data('test_cifar_dataset', 1,7)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(t[i][0].numpy().transpose(1,2,0))\n",
    "    plt.title('Original')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(t[i][2].numpy().transpose(1,2,0))\n",
    "    plt.title('Adversary')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.load('test_cifar_dataset.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (t['original_labels']==1) * (t['target_labels']==7)\n",
    "r0 = t['original_images'][mask]\n",
    "r1 = t['adversarial_images'][mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(r0[i].transpose(1,2,0))\n",
    "    plt.title('Original')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(r1[i].transpose(1,2,0))\n",
    "    plt.title('Adversary')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = utils.get_adv_data('test_cifar_dataset', 9, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = iter(t).next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1172796a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "i = 13\n",
    "plt.imshow(s[0][i].numpy().transpose(1,2,0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cifar_adv = Adversarial_utils('cifar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cifar_adv.fool_calibrate(8, epsilon=1e-1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_adv = Adversarial_utils('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist_adv.fool_calibrate(target=4, epsilon=1e-2, num_iters=100, max_L2_norm=3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
