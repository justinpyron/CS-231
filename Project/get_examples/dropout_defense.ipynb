{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNIST_adv(Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader for MNIST adversarial images.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 original_label,\n",
    "                 target_label):\n",
    "        \"\"\" Intialize the MNIST dataset\n",
    "        \n",
    "        Args:\n",
    "            - original_label: the label of natural image that \n",
    "                was used to create adversarial image\n",
    "            - target_label: the label that the model was tricked\n",
    "                into predicting\n",
    "        \"\"\"\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        self.filenames = []\n",
    "        \n",
    "        # read filenames\n",
    "        path = os.path.join('MNIST_adversarial_FGVM', \n",
    "                                 'original_{}'.format(original_label), \n",
    "                                 'image*', \n",
    "                                 'target_{}.png'.format(target_label))\n",
    "        filenames = glob.glob(path)\n",
    "        \n",
    "        for fn in filenames:\n",
    "            # (filename, original_label, target_label) tuple\n",
    "            self.filenames.append((fn, original_label, target_label))\n",
    "                    \n",
    "        self.len = len(self.filenames)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        image_fn, original_label, target_label = self.filenames[index]\n",
    "        image = Image.open(image_fn)\n",
    "        \n",
    "        # Transform image into Torch tensor\n",
    "        # Also makes range between 0 and 1 (instead of 0 to 255)\n",
    "        image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # return image, original_label, and target_label\n",
    "        return image, original_label, target_label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niZ1Je0BuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116541400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Form a dataset of adversarial examples\n",
    "\n",
    "# dataset of adversarial images originally of class 1 but targeted to class 3\n",
    "data = MNIST_adv(1,3)\n",
    "# trainset_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "# Plot a few examples\n",
    "j = 20\n",
    "test_image = data[j][0]\n",
    "plt.imshow(test_image[0], cmap='binary_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define architecture of dropout-defended model\n",
    "This model should have same architecture as model for which adversarial images were generated -- except this model will have dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(20, 20, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(20*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d( self.conv2(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d( self.conv4(self.conv3(x)), 2))\n",
    "        x = x.view(-1, 20*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dropout_Net(nn.Module):\n",
    "    def __init__(self, dropout_prob):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.drop_prob = dropout_prob\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(20, 20, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(20*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         # First conv layer\n",
    "#         x = F.dropout(self.conv1(x), p=self.dropout_prob, training=self.training)\n",
    "#         x = F.dropout(self.conv2(x), p=self.dropout_prob, training=self.training)\n",
    "#         x = F.relu(F.max_pool2d(x,2))\n",
    "        \n",
    "#         # Second conv layer\n",
    "#         x = F.dropout(self.conv3(x), p=self.dropout_prob, training=self.training)\n",
    "#         x = F.dropout(self.conv4(x), p=self.dropout_prob, training=self.training)\n",
    "#         x = F.relu(F.max_pool2d(x,2))\n",
    "\n",
    "#         # FC conv layer\n",
    "#         x = x.view(-1, 20*4*4)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        \n",
    "#         return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        x = F.relu(F.max_pool2d( self.conv2(self.conv1(x)), 2))\n",
    "        x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        \n",
    "        x = F.relu(F.max_pool2d( self.conv4(self.conv3(x)), 2))\n",
    "        x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "\n",
    "        x = x.view(-1, 20*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        \n",
    "        # original\n",
    "#         x = F.relu(F.max_pool2d( self.conv2(self.conv1(x)), 2))\n",
    "#         x = F.relu(F.max_pool2d( self.conv4(self.conv3(x)), 2))\n",
    "#         x = x.view(-1, 20*4*4)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the MNIST dataset. \n",
    "# transforms.ToTensor() automatically converts PIL images to\n",
    "# torch tensors with range [0, 1]\n",
    "trainset = MNIST(\n",
    "    root='mnist_png/training',\n",
    "    preload=True, transform=transforms.ToTensor(),\n",
    ")\n",
    "# Use the torch dataloader to iterate through the dataset\n",
    "trainset_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "# load the testset\n",
    "testset = MNIST(\n",
    "    root='mnist_png/testing',\n",
    "    preload=True, transform=transforms.ToTensor(),\n",
    ")\n",
    "# Use the torch dataloader to iterate through the dataset\n",
    "testset_loader = DataLoader(testset, batch_size=1000, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainset))\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainset_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j] for j in range(16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(cuda if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Conv Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=100):\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()  # set evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in testset_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(testset_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testset_loader.dataset),\n",
    "        100. * correct / len(testset_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(5)  # train 5 epochs should get you to about 97% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save the model (model checkpointing)\n",
    "\n",
    "Now we have a model! Obviously we do not want to retrain the model everytime we want to use it. Plus if you are training a super big model, you probably want to save checkpoint periodically so that you can always fall back to the last checkpoint in case something bad happened or you simply want to test models at different training iterations.\n",
    "\n",
    "Model checkpointing is fairly simple in PyTorch. First, we define a helper function that can save a model to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = {'state_dict': model.state_dict(),\n",
    "             'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a brand new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a training loop with model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_save(epoch, save_interval, log_interval=100):\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            if iteration % save_interval == 0 and iteration > 0:\n",
    "                save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)\n",
    "            iteration += 1\n",
    "        test()\n",
    "    \n",
    "    # save the final model\n",
    "    save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_save(5, 500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# load from the final checkpoint\n",
    "load_checkpoint('mnist-4690.pth', model, optimizer)\n",
    "# should give you the final model accuracy\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune a model\n",
    "\n",
    "Sometimes you want to fine-tune a pretrained model instead of training a model from scratch. For example, if you want to train a model on a new dataset that contains natural images. To achieve the best performance, you can start with a model that's fully trained on ImageNet and fine-tune the model.\n",
    "\n",
    "Finetuning a model in PyTorch is super easy! First, let's find out what we saved in a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What's in a state dict?\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the fc layers\n",
    "\n",
    "Now say we want to load the conv layers from the checkpoint and train the fc layers. We can simply load a subset of the state dict with the selected names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('mnist-4690.pth')\n",
    "states_to_load = {}\n",
    "for name, param in checkpoint['state_dict'].items():\n",
    "    if name.startswith('conv'):\n",
    "        states_to_load[name] = param\n",
    "\n",
    "# Construct a new state dict in which the layers we want\n",
    "# to import from the checkpoint is update with the parameters\n",
    "# from the checkpoint\n",
    "model_state = model.state_dict()\n",
    "model_state.update(states_to_load)\n",
    "        \n",
    "model = Net().to(device)\n",
    "model.load_state_dict(model_state)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(1)  # training 1 epoch will get you to 93%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pretrained weights in a different model\n",
    "\n",
    "We can even use the pretrained conv layers in a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = SmallNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('mnist-4690.pth')\n",
    "states_to_load = {}\n",
    "for name, param in checkpoint['state_dict'].items():\n",
    "    if name.startswith('conv'):\n",
    "        states_to_load[name] = param\n",
    "\n",
    "# Construct a new state dict in which the layers we want\n",
    "# to import from the checkpoint is update with the parameters\n",
    "# from the checkpoint\n",
    "model_state = model.state_dict()\n",
    "model_state.update(states_to_load)\n",
    "        \n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(1)  # training 1 epoch will get you to 93%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
